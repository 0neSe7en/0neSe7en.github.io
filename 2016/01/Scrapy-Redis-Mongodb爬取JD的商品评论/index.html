<!doctype html>
<html class="theme-next   use-motion ">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  <link href="//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">



<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=0.4.5.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="python," />





  <link rel="alternate" href="/atom.xml" title="0neSe7en's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.2" />






<meta name="description" content="最近在学校的一个项目中，需要爬取京东的各个商品的评论，然后进行分析。但是看到国内关于Scrapy的资料不是很多，所以就想要把我做的过程写下来。因为自己做的，难以避免一些野路子，希望大大们指出更好的方法。
整体思路：
先抓取某一类商品的所有的商品ID，存入Redis
从Redis取出商品ID，然后爬取此商品所有评论，存入Mongodb。

分析页面商品ID列表打开笔记本电脑的分类：http://li">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy+Redis+Mongodb爬取JD的商品评论">
<meta property="og:url" content="http://0nese7en.github.io/2016/01/Scrapy-Redis-Mongodb爬取JD的商品评论/index.html">
<meta property="og:site_name" content="0neSe7en's Blog">
<meta property="og:description" content="最近在学校的一个项目中，需要爬取京东的各个商品的评论，然后进行分析。但是看到国内关于Scrapy的资料不是很多，所以就想要把我做的过程写下来。因为自己做的，难以避免一些野路子，希望大大们指出更好的方法。
整体思路：
先抓取某一类商品的所有的商品ID，存入Redis
从Redis取出商品ID，然后爬取此商品所有评论，存入Mongodb。

分析页面商品ID列表打开笔记本电脑的分类：http://li">
<meta property="og:image" content="http://0nese7en.github.io/2016/01/Scrapy-Redis-Mongodb爬取JD的商品评论/single_skuid.png">
<meta property="og:image" content="http://0nese7en.github.io/2016/01/Scrapy-Redis-Mongodb爬取JD的商品评论/max_pages.png">
<meta property="og:image" content="http://0nese7en.github.io/2016/01/Scrapy-Redis-Mongodb爬取JD的商品评论/fetch_skuid.png">
<meta property="og:image" content="http://0nese7en.github.io/2016/01/Scrapy-Redis-Mongodb爬取JD的商品评论/comments.png">
<meta property="og:updated_time" content="2016-01-08T16:34:43.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Scrapy+Redis+Mongodb爬取JD的商品评论">
<meta name="twitter:description" content="最近在学校的一个项目中，需要爬取京东的各个商品的评论，然后进行分析。但是看到国内关于Scrapy的资料不是很多，所以就想要把我做的过程写下来。因为自己做的，难以避免一些野路子，希望大大们指出更好的方法。
整体思路：
先抓取某一类商品的所有的商品ID，存入Redis
从Redis取出商品ID，然后爬取此商品所有评论，存入Mongodb。

分析页面商品ID列表打开笔记本电脑的分类：http://li">



<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: '',
    sidebar: 'post',
    motion: true
  };
</script>

  <title> Scrapy+Redis+Mongodb爬取JD的商品评论 | 0neSe7en's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  



  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?fdd4f8b2bd53186a3fa40ee9630b6f75";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">0neSe7en's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">编程 & 吹水</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th fa-fw"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br />
            
            标签
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content">
          

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Scrapy+Redis+Mongodb爬取JD的商品评论
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            发表于
            <time itemprop="dateCreated" datetime="2016-01-08T23:42:12+08:00" content="2016-01-08">
              2016-01-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分类于
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Program/" itemprop="url" rel="index">
                    <span itemprop="name">Program</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/01/Scrapy-Redis-Mongodb爬取JD的商品评论/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/01/Scrapy-Redis-Mongodb爬取JD的商品评论/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><p>最近在学校的一个项目中，需要爬取京东的各个商品的评论，然后进行分析。但是看到国内关于Scrapy的资料不是很多，所以就想要把我做的过程写下来。因为自己做的，难以避免一些野路子，希望大大们指出更好的方法。</p>
<h2 id="u6574_u4F53_u601D_u8DEF_uFF1A"><a href="#u6574_u4F53_u601D_u8DEF_uFF1A" class="headerlink" title="整体思路："></a>整体思路：</h2><ul>
<li>先抓取某一类商品的所有的商品ID，存入Redis</li>
<li>从Redis取出商品ID，然后爬取此商品所有评论，存入Mongodb。</li>
</ul>
<h2 id="u5206_u6790_u9875_u9762"><a href="#u5206_u6790_u9875_u9762" class="headerlink" title="分析页面"></a>分析页面</h2><h3 id="u5546_u54C1ID_u5217_u8868"><a href="#u5546_u54C1ID_u5217_u8868" class="headerlink" title="商品ID列表"></a>商品ID列表</h3><p>打开笔记本电脑的分类：<a href="http://list.jd.com/list.html?cat=670,671,672" target="_blank" rel="external">http://list.jd.com/list.html?cat=670,671,672</a>，其中url里面的<code>cat=670,671,672</code>就是商品类别，然后页面上的每一个笔记本电脑都会链接到它自己的详情页面：<a href="http://item.jd.com/1592705.html" target="_blank" rel="external">http://item.jd.com/1592705.html</a>，url中的<code>1466274</code>就是商品ID，打开详情页面以后，确认这个数字就是商品ID了。</p>
<img src="/2016/01/Scrapy-Redis-Mongodb爬取JD的商品评论/single_skuid.png" alt="single_skuid" title="single_skuid">
<p>所以，想要获取某一类别的所有商品ID，只需要通过商品列表页面，就可以把一页的商品ID取出来。然后，再翻页，直到没有新的商品ID出现。或者通过页面中的最大页数来判断。</p>
<img src="/2016/01/Scrapy-Redis-Mongodb爬取JD的商品评论/max_pages.png" alt="max_pages.png" title="">
<p>找到了数据的位置以后，就是如何把它们提取出来了。</p>
<h4 id="u63D0_u53D6_u6570_u636E"><a href="#u63D0_u53D6_u6570_u636E" class="headerlink" title="提取数据"></a>提取数据</h4><img src="/2016/01/Scrapy-Redis-Mongodb爬取JD的商品评论/fetch_skuid.png" alt="fetch_skuid.png" title="">
<p>商品的ID，可以看到，是在<code>div</code>下面的<code>data-sku</code>属性`。所以可以通过正则，来把整页的商品ID取出来。</p>
<pre><code>data-sku=&quot;(\d+)&quot;
</code></pre><p>用同样的方法，可以看到最大页数的地方是这样的：</p>
<pre><code>&lt;span class=&quot;fp-text&quot;&gt;&lt;b&gt;1&lt;/b&gt;&lt;em&gt;/&lt;/em&gt;&lt;i&gt;208&lt;/i&gt;&lt;/span&gt;
</code></pre><p>所以继续用正则（这个地方用XPATH感觉更好），来将其中的208取出来</p>
<pre><code>fp-text.+?&lt;i&gt;(\d+)&lt;/i&gt;
</code></pre><h3 id="u67D0_u4E2A_u5546_u54C1_u7684_u8BC4_u8BBA_u4FE1_u606F"><a href="#u67D0_u4E2A_u5546_u54C1_u7684_u8BC4_u8BBA_u4FE1_u606F" class="headerlink" title="某个商品的评论信息"></a>某个商品的评论信息</h3><p>点进一个商品详情，随便选取一段评论，在网页的源码中无法查到，说明评论数据是之后加载的。打开开发者工具中的Network页面，点击下一页的评论，可以看到如下：</p>
<img src="/2016/01/Scrapy-Redis-Mongodb爬取JD的商品评论/comments.png" alt="comments.png" title="">
<p>其中被我选中的这个请求，就是获得这个商品评论的方式。也就是：<code>http://s.club.jd.com/productpage/p-[skuid]-s-0-t-0-p-[page].html</code>。</p>
<h2 id="u7B2C_u4E00_u6B21_u5C1D_u8BD5"><a href="#u7B2C_u4E00_u6B21_u5C1D_u8BD5" class="headerlink" title="第一次尝试"></a>第一次尝试</h2><a id="more"></a>
<p>上述内容中，已经知道了如何获得某一类商品的ID，以及知道如何获得商品的评论数据了。所以，就可以开始最基本的爬虫的编写了。但是编写前，有一个事情需要确定，那就是京东的<strong>反爬虫</strong>机制。所以，写了两个简单的脚本，来获取相应的数据。</p>
<h3 id="u83B7_u53D6_u5546_u54C1_u5217_u8868_u7684_u4EE3_u7801"><a href="#u83B7_u53D6_u5546_u54C1_u5217_u8868_u7684_u4EE3_u7801" class="headerlink" title="获取商品列表的代码"></a>获取商品列表的代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">base_url = <span class="string">r'http://list.jd.com/list.html?cat=670,671,672%s'</span> <span class="comment">#列表的基本的url</span></span><br><span class="line">page = <span class="string">r'&amp;page=%d'</span></span><br><span class="line"></span><br><span class="line">skuids = set() <span class="comment">#保证商品ID是唯一的</span></span><br><span class="line">first_try = requests.get(base_url)</span><br><span class="line">sku_re = re.compile(<span class="string">r'data-sku="(\d+)"'</span>, re.MULTILINE | re.IGNORECASE)</span><br><span class="line">ids = re.findall(sku_re, first_try.text)</span><br><span class="line">print(ids)</span><br><span class="line">print(<span class="string">'find...'</span>, len(ids))</span><br><span class="line">skuids |= set(ids)</span><br><span class="line"></span><br><span class="line">i = <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    url = base_url % (page % i)</span><br><span class="line">    html = requests.get(url)</span><br><span class="line">    ids = set(re.findall(sku_re, html.text))</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">193</span> <span class="keyword">or</span> len(ids) == <span class="number">0</span> <span class="keyword">or</span> len(skuids - ids) == <span class="number">0</span>: <span class="comment">#这个地方...是因为偷懒，所以将具体的页数直接写了出来</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    skuids |= set(ids)</span><br><span class="line">    total = len(skuids)</span><br><span class="line">    print(<span class="string">'Total:'</span>, total)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'skuids.txt'</span>, mode=<span class="string">'w'</span>) <span class="keyword">as</span> s:</span><br><span class="line">    <span class="keyword">for</span> sku <span class="keyword">in</span> skuids:</span><br><span class="line">        s.write(sku + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<p>这个脚本用了Requests库，使用起来十分方便。因为当时只是简单的测试脚本，所以有的地方写的十分偷懒…整体的思路就是，遍历列表页，通过正则取出此页面的商品ID，放到一个集合中。最后把集合写到一个文件里，方便之后抓取评论数据。</p>
<h3 id="u83B7_u53D6_u5546_u54C1_u8BC4_u8BBA_u7684_u4EE3_u7801"><a href="#u83B7_u53D6_u5546_u54C1_u8BC4_u8BBA_u7684_u4EE3_u7801" class="headerlink" title="获取商品评论的代码"></a>获取商品评论的代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> simplejson <span class="keyword">as</span> json <span class="comment">#感觉simplejson这个库比原生的json库快了很多</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">base_url = <span class="string">r'http://s.club.jd.com/productpage/p-%s-s-0-t-0-p-%d.html'</span> <span class="comment">#评论的base_url</span></span><br><span class="line">results = open(<span class="string">'skuid_comments.json'</span>, mode=<span class="string">'a'</span>) <span class="comment">#将评论写入此文件</span></span><br><span class="line">skuid_file = open(<span class="string">'skuids.txt'</span>, mode=<span class="string">'r'</span>) <span class="comment">#从商品ID文件中读取</span></span><br><span class="line">user_agents_file = open(<span class="string">'uas.txt'</span>, mode=<span class="string">'r'</span>) <span class="comment">#一个里面有很多User Agent的文件</span></span><br><span class="line">current_progress = open(<span class="string">'progress'</span>, mode=<span class="string">'r'</span>) <span class="comment">#如果中间退出了，则进度保存在此文件</span></span><br><span class="line">progress = current_progress.read()</span><br><span class="line">current_skuid = <span class="keyword">None</span></span><br><span class="line">current_page = <span class="keyword">None</span></span><br><span class="line"><span class="comment"># 如果有进度的话，则从进度位置开始</span></span><br><span class="line"><span class="keyword">if</span> progress: </span><br><span class="line">    current_skuid = progress.strip().split(<span class="string">' '</span>)[<span class="number">0</span>].strip()</span><br><span class="line">    current_page = progress.strip().split(<span class="string">' '</span>)[<span class="number">1</span>].strip()</span><br><span class="line"></span><br><span class="line">ua_list = [x.strip() <span class="keyword">for</span> x <span class="keyword">in</span> user_agents_file.readlines()]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> skuid_str <span class="keyword">in</span> skuid_file.readlines():</span><br><span class="line">    <span class="keyword">if</span> current_skuid:</span><br><span class="line">        <span class="keyword">if</span> skuid_str.strip() != current_skuid:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        page = int(current_page)</span><br><span class="line">        current_skuid = <span class="keyword">None</span></span><br><span class="line">        current_page = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        page = <span class="number">0</span></span><br><span class="line">    skuid = skuid_str.strip()</span><br><span class="line">    print(<span class="string">'Current Skuid:'</span>, skuid)</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        sec = random.randint(<span class="number">1</span>, <span class="number">4</span>) <span class="comment"># 随机延时1~4秒，防止被Ban</span></span><br><span class="line">        time.sleep(sec)</span><br><span class="line">        ua = random.choice(ua_list) <span class="comment"># 从UA的列表中选择一个</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            comments_json = requests.get(base_url % (skuid, page), headers=&#123;<span class="string">'User-Agent'</span>: ua&#125;)</span><br><span class="line">            print(comments_json.request.headers)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="comment"># 这个地方，是当用户使用Ctrl+C的时候，或者requests超时的时候，将当前的进度写到文件中</span></span><br><span class="line">            <span class="keyword">with</span> open(<span class="string">'progress'</span>, mode=<span class="string">'w'</span>) <span class="keyword">as</span> p:</span><br><span class="line">                p.write(skuid + <span class="string">' '</span> + str(page))</span><br><span class="line">            time.sleep(<span class="number">180</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> comments_json.text:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        comments = json.loads(comments_json.text)</span><br><span class="line">        <span class="keyword">if</span> len(comments[<span class="string">'comments'</span>]): <span class="comment"># 如果comments没有内容了，则爬取下一个商品</span></span><br><span class="line">            results.write(comments_json.text + <span class="string">'\n'</span>)</span><br><span class="line">            page += <span class="number">1</span></span><br><span class="line">            print(<span class="string">'Page: '</span>, page)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>用上述代码跑了一段时间，发现京东的反爬虫机制还是比较松的，也可能是因为我的频率太慢。然后…基本的测试结束之后，就开始用scrapy了。</p>
<h2 id="u4F7F_u7528Scrapy"><a href="#u4F7F_u7528Scrapy" class="headerlink" title="使用Scrapy"></a>使用Scrapy</h2><p>初始化项目，然后新建两个Spider。项目目录结构如下：</p>
<pre><code>├── jd_comments
│   ├── __init__.py
│   ├── items.py
│   ├── pipelines.py
│   ├── settings.py
│   └── spiders
│       ├── __init__.py
│       ├── comments.py
│       └── skuid_list.py
└── scrapy.cfg        └── scrapy.cfg
</code></pre><p>即创建两个spider：<code>skuid_list</code>和<code>comments</code>。然后，<code>items.py</code>的代码就是这个：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdCommentsItem</span><span class="params">(scrapy.Item)</span>:</span>       </span><br><span class="line">    <span class="keyword">pass</span> <span class="comment"># 暂不说comment</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SkuIdItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    product_id = scrapy.Field() <span class="comment">#只有一个商品ID字段</span></span><br></pre></td></tr></table></figure>
<h3 id="skuid_list-py"><a href="#skuid_list-py" class="headerlink" title="skuid_list.py"></a>skuid_list.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> jd_comments.items <span class="keyword">import</span> SkuIdItem</span><br><span class="line"><span class="keyword">from</span> jd_comments <span class="keyword">import</span> pipelines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SkuidListSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"skuid_list"</span></span><br><span class="line">    allowed_domains = [<span class="string">"jd.com"</span>]</span><br><span class="line">    base_url = <span class="string">'http://list.jd.com/list.html?cat=%s'</span></span><br><span class="line">    page = <span class="string">'&amp;page=%d'</span></span><br><span class="line">    p = re.compile(<span class="string">ur'fp-text.+?&lt;i&gt;(\d+)&lt;/i&gt;'</span>, re.MULTILINE | re.IGNORECASE)</span><br><span class="line">    sku_re = re.compile(<span class="string">r'data-sku="(\d+)"'</span>, re.MULTILINE | re.IGNORECASE)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cid=<span class="string">'670,671,672'</span>, *args, **kwargs)</span>:</span> <span class="comment"># 初始化的时候，设置类别的ID。可以在启动爬虫的时候赋予参数，设置分类的ID</span></span><br><span class="line">        super(SkuidListSpider, self).__init__(*args, **kwargs)</span><br><span class="line">        self.cid = cid</span><br><span class="line">        self.start_urls = []</span><br><span class="line">        self.max_page = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span> <span class="comment"># 如果start_urls为空的话，会调用此函数。必须返回一组Request。</span></span><br><span class="line">        print(<span class="string">'Into start requests...'</span>)</span><br><span class="line">        <span class="keyword">return</span> [scrapy.Request(self.base_url % self.cid,</span><br><span class="line">                               callback=self.get_max_pages)] <span class="comment">#在这里，获取了最大的页数。所以callback为get_max_pages</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_max_pages</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.logger.info(<span class="string">'get max pages: %s. CID: %s'</span>, response.url, self.cid)</span><br><span class="line">        self.max_page = int(self.p.search(response.body).group(<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(response.url, callback=self.parse) <span class="comment"># 抓取第一页</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, self.max_page + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(response.url + (self.page % i)) <span class="comment">#抓取之后的每一页...</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.logger.info(<span class="string">'get page: %s'</span>, response.url)</span><br><span class="line">        id_list = self.sku_re.findall(response.body)</span><br><span class="line">        <span class="keyword">for</span> skuid <span class="keyword">in</span> id_list:</span><br><span class="line">            <span class="keyword">yield</span> SkuIdItem(product_id=skuid) <span class="comment">#根据抓取的页面，获取到这些商品的ID，并生成item</span></span><br></pre></td></tr></table></figure>
<p>现在的商品ID爬虫，只能够把ID输出来，就没有然后了…所以需要使用<code>pipeline</code>把找到的这些商品ID都放入redis中。但是，一般情况下，pipeline都是针对所有爬虫的。而对于商品ID需要存入Redis，而商品评论只需要存入Mongodb里面。所以，还需要一个用来检测不同爬虫执行不同pipeline的函数。</p>
<h3 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a>pipelines.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">from</span> jd_comments <span class="keyword">import</span> redis_pool</span><br><span class="line"></span><br><span class="line">r = redis.Redis(connection_pool=redis_pool) <span class="comment">#创建一个redis客户端，放入连接池中。</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_spider_pipeline</span><span class="params">(process_item_method)</span>:</span> <span class="comment">#判断爬虫执行那些脚本的装饰器</span></span><br><span class="line"><span class="decorator">    @functools.wraps(process_item_method)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># message template for debugging</span></span><br><span class="line">        msg = <span class="string">'%%s %s pipeline step'</span> % (self.__class__.__name__,)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if class is in the spider's pipeline, then use the</span></span><br><span class="line">        <span class="comment"># process_item method normally.</span></span><br><span class="line">        <span class="keyword">if</span> self.__class__ <span class="keyword">in</span> spider.pipeline:</span><br><span class="line">            spider.logger.debug(msg % <span class="string">'executing'</span>)</span><br><span class="line">            <span class="keyword">return</span> process_item_method(self, item, spider)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># otherwise, just return the untouched item (skip this step in</span></span><br><span class="line">        <span class="comment"># the pipeline)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            spider.logger.debug(msg % <span class="string">'skipping'</span>)</span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SkuidRedisPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"><span class="decorator">    @check_spider_pipeline</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        r.sadd(<span class="string">'comments:queue'</span>, item[<span class="string">'product_id'</span>]) <span class="comment">#将商品ID放入队列中</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>有了这个代码以后，为了应用这个<code>pipeline</code>，需要在<code>settings.py</code>里面设置。并且，在刚才的<code>skuid_list.py</code>中加入如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">....</span><br><span class="line">sku_re = re.compile(<span class="string">r'data-sku="(\d+)"'</span>, re.MULTILINE | re.IGNORECASE)</span><br><span class="line">pipeline = set([pipelines.SkuidRedisPipeline]) <span class="comment">#指定skuid_list的pipeline为这个...</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cid=<span class="string">'670,671,672'</span>, *args, **kwargs)</span>:</span></span><br><span class="line">....</span><br></pre></td></tr></table></figure>
<p><em>(转载请注明出处)</em></p>
</span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag">#python</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/01/hello-world/" rel="next" title="Hello World">
                <i class="fa fa-chevron-left"></i> Hello World
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <a class="jiathis_button_tsina"></a>
  <a class="jiathis_button_tqq"></a>
  <a class="jiathis_button_weixin"></a>
  <a class="jiathis_button_cqq"></a>
  <a class="jiathis_button_douban"></a>
  <a class="jiathis_button_renren"></a>
  <a class="jiathis_button_qzone"></a>
  <a class="jiathis_button_kaixin001"></a>
  <a class="jiathis_button_copy"></a>
  <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
  var jiathis_config={
    hideMore:false
  }
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

      
    </div>
  </div>


        </div>

        

  <p>热评文章</p>
  <div class="ds-top-threads" data-range="weekly" data-num-items="4"></div>


        
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2016/01/Scrapy-Redis-Mongodb爬取JD的商品评论/"
           data-title="Scrapy+Redis+Mongodb爬取JD的商品评论" data-url="http://0nese7en.github.io/2016/01/Scrapy-Redis-Mongodb爬取JD的商品评论/">
      </div>
    
  </div>


      </div>

      
        
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/default_avatar.jpg" alt="0neSe7en" itemprop="image"/>
          <p class="site-author-name" itemprop="name">0neSe7en</p>
        </div>
        <p class="site-description motion-element" itemprop="description"></p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">2</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">1</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">1</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/0neSe7en/" target="_blank">
                  
                    <i class="fa fa-github"></i> GitHub
                  
                </a>
              </span>
            
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator">
            <i class="fa fa-angle-double-up"></i>
          </div>
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#u6574_u4F53_u601D_u8DEF_uFF1A"><span class="nav-number">1.</span> <span class="nav-text">整体思路：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#u5206_u6790_u9875_u9762"><span class="nav-number">2.</span> <span class="nav-text">分析页面</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#u5546_u54C1ID_u5217_u8868"><span class="nav-number">2.1.</span> <span class="nav-text">商品ID列表</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#u63D0_u53D6_u6570_u636E"><span class="nav-number">2.1.1.</span> <span class="nav-text">提取数据</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#u67D0_u4E2A_u5546_u54C1_u7684_u8BC4_u8BBA_u4FE1_u606F"><span class="nav-number">2.2.</span> <span class="nav-text">某个商品的评论信息</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#u7B2C_u4E00_u6B21_u5C1D_u8BD5"><span class="nav-number">3.</span> <span class="nav-text">第一次尝试</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#u83B7_u53D6_u5546_u54C1_u5217_u8868_u7684_u4EE3_u7801"><span class="nav-number">3.1.</span> <span class="nav-text">获取商品列表的代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#u83B7_u53D6_u5546_u54C1_u8BC4_u8BBA_u7684_u4EE3_u7801"><span class="nav-number">3.2.</span> <span class="nav-text">获取商品评论的代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#u4F7F_u7528Scrapy"><span class="nav-number">4.</span> <span class="nav-text">使用Scrapy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#skuid_list-py"><span class="nav-number">4.1.</span> <span class="nav-text">skuid_list.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pipelines-py"><span class="nav-number">4.2.</span> <span class="nav-text">pipelines.py</span></a></li></ol></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator">
            <i class="fa fa-angle-double-down"></i>
          </div>
        </section>
      

    </div>
  </aside>


      
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="icon-next-heart fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">0neSe7en</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>



      </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  

  
    

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"se7en"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>
    
     


    
  

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.2"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.2"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
<script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

<script type="text/javascript" src="/js/motion.js?v=0.4.5.2" id="motion.global"></script>


  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.2" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 1 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    motionMiddleWares.sidebar = function () {
      var $tocContent = $('.post-toc-content');
      if (CONFIG.sidebar === 'post') {
        if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
          displaySidebar();
        }
      }
    };
  });
</script>



  <script type="text/javascript" src="/js/bootstrap.js"></script>

  
  

  
  

</body>
</html>
